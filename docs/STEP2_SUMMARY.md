# Step 2 Summary: Data Collection & Preprocessing

**Status**: âœ… COMPLETE

---

## What Was Accomplished

### 1. MediaPipe Hand Detector Wrapper ([src/preprocessing/hand_detector.py](../src/preprocessing/hand_detector.py))

**Purpose**: Clean interface for MediaPipe hand detection

**Key Features**:
- Detects hands in images/video frames
- Extracts 21 landmarks with (x, y, z) coordinates
- Provides both normalized (0-1) and pixel coordinates
- Visualizes landmarks and connections
- Calculates bounding boxes around hands
- Context manager support for resource cleanup

**Academic Justification**:
- MediaPipe uses two-stage CNN pipeline (palm detection + landmark regression)
- Real-time performance (>30 FPS on CPU)
- Published by Google Research with extensive validation
- Reference: Zhang et al., "MediaPipe Hands" (2020)

**Methods**:
```python
detector = HandDetector()
success, results = detector.detect_hands(frame)
landmarks = detector.get_landmarks(results)  # (21, 3) array
landmarks_pixel = detector.get_landmarks_pixel_coords(results, frame.shape)
annotated_frame = detector.draw_landmarks(frame, results)
```

---

### 2. Feature Extraction Module ([src/preprocessing/feature_extractor.py](../src/preprocessing/feature_extractor.py))

**Purpose**: Convert raw landmarks into normalized feature vectors

**Key Features**:
- **Wrist-relative normalization** (recommended):
  - Translates all landmarks relative to wrist
  - Scales by hand size (wrist to middle finger MCP)
  - Provides scale and position invariance
- **Bounding box normalization** (alternative):
  - Normalizes to [0, 1] range based on bbox
  - More rotation-invariant
- **Optional features**:
  - Distance features (inter-landmark distances)
  - Angle features (joint angles)

**Feature Vector**:
- **Basic**: 63 dimensions (21 landmarks Ã— 3 coords)
- Flattened: [xâ‚€, yâ‚€, zâ‚€, xâ‚, yâ‚, zâ‚, ..., xâ‚‚â‚€, yâ‚‚â‚€, zâ‚‚â‚€]

**Academic Justification**:
- Normalization critical for robustness
- Wrist-relative method preserves hand shape
- Scale invariance: works with different hand sizes
- Position invariance: hand location doesn't affect features

**Usage**:
```python
extractor = FeatureExtractor(normalization_method='wrist')
features = extractor.extract_features(landmarks)  # (63,) array
```

---

### 3. Static Gesture Data Collection ([src/data_collection/collect_static.py](../src/data_collection/collect_static.py))

**Purpose**: Interactive tool for collecting static (single-frame) gesture data

**Features**:
- Real-time webcam interface with visual feedback
- Progress tracking with resume capability
- Interactive controls (SPACE to capture, ENTER for next)
- Automatic feature extraction and saving
- Progress bar visualization

**Dataset Structure**:
```
data/raw/static/
  â”œâ”€â”€ A/
  â”‚   â”œâ”€â”€ landmarks_20260112_120534_123456.npy
  â”‚   â””â”€â”€ ...
  â”œâ”€â”€ B/
  â””â”€â”€ ...

data/processed/
  â”œâ”€â”€ static_features.npy      # (N, 63) array
  â”œâ”€â”€ static_labels.npy         # (N,) array

data/labels/
  â””â”€â”€ static_label_mapping.json # {0: 'A', 1: 'B', ...}
```

**Controls**:
- `SPACE`: Capture current frame
- `ENTER`: Move to next gesture
- `R`: Reset progress
- `ESC`: Quit

**Academic Justification**:
- Large dataset (300 samples Ã— 36 classes = 10,800 samples)
- Multiple samples capture natural variation
- Systematic collection ensures balanced dataset
- Feature extraction during collection saves processing time

**Usage**:
```bash
python src/data_collection/collect_static.py
```

---

### 4. Dynamic Gesture Data Collection ([src/data_collection/collect_dynamic.py](../src/data_collection/collect_dynamic.py))

**Purpose**: Interactive tool for collecting dynamic (motion-based) gesture sequences

**Features**:
- Countdown timer before recording
- Automatic 30-frame sequence capture
- Real-time progress bar during recording
- Missing frame interpolation
- Sequence validation (70% valid frames required)

**Dataset Structure**:
```
data/raw/dynamic/
  â”œâ”€â”€ hello/
  â”‚   â”œâ”€â”€ sequence_20260112_120534_123456.npy  # (30, 21, 3)
  â”‚   â””â”€â”€ ...
  â”œâ”€â”€ thanks/
  â””â”€â”€ ...

data/processed/
  â”œâ”€â”€ dynamic_sequences.npy     # (N, 30, 63) array
  â”œâ”€â”€ dynamic_labels.npy        # (N,) array

data/labels/
  â””â”€â”€ dynamic_label_mapping.json
```

**Sequence Recording Process**:
1. Press SPACE to start
2. 3-second countdown
3. Perform gesture
4. Automatic capture for 30 frames (~1 second)
5. Validation and interpolation
6. Save if valid (>70% frames with hand detected)

**Controls**:
- `SPACE`: Start recording sequence
- `ENTER`: Move to next gesture
- `R`: Reset progress
- `ESC`: Quit

**Academic Justification**:
- Fixed sequence length (30 frames) simplifies BiLSTM input
- Temporal data captures motion patterns
- Multiple sequences per class (100) capture speed/style variation
- Linear interpolation handles occasional missing frames

**Usage**:
```bash
python src/data_collection/collect_dynamic.py
```

---

## Technical Details

### Normalization Methods

#### Wrist-Relative (Recommended)
```
For landmark i:
  x_norm = (x_i - x_wrist) / hand_size
  y_norm = (y_i - y_wrist) / hand_size
  z_norm = (z_i - z_wrist) / hand_size

where hand_size = ||landmark_9 - landmark_0||
```

**Benefits**:
- âœ… Scale invariance (different hand sizes)
- âœ… Position invariance (hand location)
- âœ… Preserves hand shape
- âœ… Fast computation

**Limitations**:
- âš ï¸ Partially rotation-variant (acceptable for ASL)

---

### MediaPipe Landmark Indices

```
0: WRIST
1-4: THUMB (CMC, MCP, IP, TIP)
5-8: INDEX FINGER (MCP, PIP, DIP, TIP)
9-12: MIDDLE FINGER (MCP, PIP, DIP, TIP)
13-16: RING FINGER (MCP, PIP, DIP, TIP)
17-20: PINKY (MCP, PIP, DIP, TIP)
```

---

### Configuration Parameters

All settings in [src/utils/config.py](../src/utils/config.py):

```python
# MediaPipe
MIN_DETECTION_CONFIDENCE = 0.7
MIN_TRACKING_CONFIDENCE = 0.5
MAX_NUM_HANDS = 1

# Data Collection
STATIC_CLASSES = ['A'-'Z', '0'-'9']  # 36 classes
DYNAMIC_CLASSES = ['hello', 'thanks', ...]  # 10 classes
STATIC_SAMPLES_PER_CLASS = 300
DYNAMIC_SEQUENCES_PER_CLASS = 100
DYNAMIC_SEQUENCE_LENGTH = 30

# Features
FEATURE_DIM = 63  # 21 landmarks Ã— 3 coords
NORMALIZATION_METHOD = 'wrist'
```

---

## Data Flow

### Static Gesture Pipeline
```
Webcam â†’ Hand Detection â†’ Landmark Extraction â†’
Normalization â†’ Feature Vector (63) â†’ Save
```

### Dynamic Gesture Pipeline
```
Webcam â†’ Sequence Capture (30 frames) â†’
Hand Detection (per frame) â†’ Landmark Extraction â†’
Interpolation (if needed) â†’ Normalization â†’
Feature Sequence (30, 63) â†’ Save
```

---

## Testing Instructions

### 1. Test Hand Detector
```bash
cd "C:\Users\Adven\OneDrive\Documents\My files\Sign_Language_Recognition"
python src/preprocessing/hand_detector.py
```

**Expected**: Webcam window showing hand landmarks and bounding box

### 2. Test Feature Extractor
```bash
python src/preprocessing/feature_extractor.py
```

**Expected**: Console output showing feature extraction with sample data

### 3. Test Static Collection
```bash
python src/data_collection/collect_static.py
```

**Expected**: Interactive GUI for capturing static gestures

### 4. Test Dynamic Collection
```bash
python src/data_collection/collect_dynamic.py
```

**Expected**: Interactive GUI with countdown and sequence recording

---

## Dataset Statistics (After Collection)

### Static Gestures
- **Classes**: 36 (A-Z, 0-9)
- **Samples per class**: 300
- **Total samples**: 10,800
- **Feature dimension**: 63
- **Data file**: `data/processed/static_features.npy` (10800, 63)

### Dynamic Gestures
- **Classes**: 10 (hello, thanks, please, sorry, yes, no, help, stop, more, finish)
- **Sequences per class**: 100
- **Total sequences**: 1,000
- **Sequence length**: 30 frames
- **Feature dimension per frame**: 63
- **Data file**: `data/processed/dynamic_sequences.npy` (1000, 30, 63)

---

## Thesis Integration

### Methodology Chapter - Section 3.3: Data Collection

**Example paragraph**:

> "Data collection was conducted using a systematic protocol to ensure dataset quality and diversity. For static gestures, 300 samples were collected for each of the 36 ASL symbols (A-Z letters and 0-9 digits), totaling 10,800 samples. Data was captured using a standard webcam (640Ã—480 resolution) with MediaPipe Hands for landmark extraction. To introduce natural variation, samples were collected across multiple sessions with varying hand positions, lighting conditions, and slight rotation angles. Each sample consists of 21 hand landmarks with 3D coordinates (x, y, z), normalized using wrist-relative scaling to achieve scale and position invariance."

### Methodology Chapter - Section 3.4: Preprocessing

**Example paragraph**:

> "Preprocessing involves normalization of raw hand landmarks to ensure model robustness. We employ wrist-relative normalization, where all landmarks are translated relative to the wrist position (landmark 0) and scaled by hand size, defined as the Euclidean distance from the wrist to the middle finger metacarpophalangeal (MCP) joint (landmark 9). This approach provides scale invariance across different hand sizes and position invariance regardless of hand location in the frame. The normalized landmarks are flattened into a 63-dimensional feature vector [xâ‚€, yâ‚€, zâ‚€, ..., xâ‚‚â‚€, yâ‚‚â‚€, zâ‚‚â‚€] which serves as input to the classification models."

---

## How This Addresses Thesis Requirements

| Requirement | Status | Evidence |
|-------------|--------|----------|
| **B. System Design** | âœ… | Modular preprocessing pipeline |
| **C. Technical Implementation** | âœ… | Working MediaPipe integration, feature extraction |
| **D. ML Methodology** | âœ… | Systematic dataset collection, normalization strategy |
| **E. Evaluation** | ðŸ”„ | Data ready for model training |
| **F. Ethics & Privacy** | âœ… | Only landmarks stored, no raw video |
| **G. Documentation** | âœ… | Comprehensive code documentation |

---

## Next Steps

**Step 3 will cover**:
1. Static gesture MLP model implementation
2. Dynamic gesture BiLSTM model implementation
3. Training pipelines with data augmentation
4. Model evaluation utilities
5. Saved trained models

**Before proceeding**, you should:
1. Collect your dataset using the data collection scripts
2. Verify the processed data files are created
3. Check dataset statistics

---

## Quick Start Guide

### Collect Your Own Dataset

**Static Gestures** (letters A-Z, numbers 0-9):
```bash
python src/data_collection/collect_static.py
```

**Dynamic Gestures** (words like "hello", "thanks"):
```bash
python src/data_collection/collect_dynamic.py
```

### Tips for Data Collection

1. **Lighting**: Collect in well-lit areas
2. **Background**: Use plain backgrounds for better detection
3. **Variation**: Vary hand position and orientation slightly
4. **Consistency**: Keep gestures consistent with ASL standards
5. **Sessions**: Collect in multiple sessions to avoid fatigue
6. **Speed** (dynamic): Perform gestures at natural speed

---

**Status**: Step 2 Complete âœ…

**Ready for**: Testing and dataset collection

**Next**: Step 3 - Model Development (after dataset collection)

---

*Last Updated: 2026-01-12*
